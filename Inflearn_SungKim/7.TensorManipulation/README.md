# TensorManipulation

## Deep Neural Network
```
사람의 뇌를 생각해서 비슷하게 만들어보자!

Dream:Thinking machine

자극이 있고 어떤값 이상이되면 활성화가 되게한다.(activation Functions)

OR,AND=>linearly separable
XOR=>not linearly separable
```

## Backpropagation 
```
실제로 틀린값을 예측했다면 Weight와 Bias를 조절해야 한다. 
error면 backward!
```

## Convolutinal Neural Networks
```
각각의 신경망들이 있고 나중에 조합되는 것이 아닐까?
부분부분을 잘라서 layer에 보내고 나중에 합친다(Alphago)
```

## Breakthrough
```
A Big problem
->Backpropagation이 앞에 Node까지 전달되지 않는문제..

Neural networks with many layers really could be trained well
->if the weights are initializer in a clever way rather than randomly
```

[참고:tensor_manipulation](https://github.com/hunkim/DeepLearningZeroToAll/blob/master/lab-08-tensor_manipulation.ipynb)